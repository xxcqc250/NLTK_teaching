{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Learning to Classify Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.   Supervised Classification\n",
    "Classification(分類)是對input來給予正確的類別標籤<br>\n",
    "在基本的分類項目中，每個input都是獨立的，標籤也事先定義<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "稱為『Supervised』的分類器是建立在訓練文集中對每個input標上正確的類別的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](https://i.imgur.com/SO0thhn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1   Gender Identification\n",
    "### 建立分類器來根據姓名的最後一個字母來判斷性別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#從nltk的文集中載入分別有男生姓名和女生姓名的文件\n",
    "from nltk.corpus import names\n",
    "import random\n",
    "import nltk\n",
    "\n",
    "#從男生姓名或女生姓名的txt檔抓出來的名字產生出一組tuple\n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] +[(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "#隨機打亂順序，不然都是排好的\n",
    "random.shuffle(labeled_names)\n",
    "\n",
    "#回傳參數最後一個字元\n",
    "def gender_features(word):\n",
    "    return {'last_letter': word[-1]}\n",
    "\n",
    "#featuresets包含tuple，分別是姓名的最後一個字元和性別\n",
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
    "\n",
    "#將featuresets前後500個data分別存成訓練集和測試集\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "\n",
    "#將訓練集使用函式NaiveBayesClassifer來產生一個新的NaiveBayes分類器\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#測試分類器\n",
    "classifier.classify(gender_features('Neo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'female'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(gender_features('Trinity'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.776\n"
     ]
    }
   ],
   "source": [
    "#訓練資料是隨機的，準確度會不一樣\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last_letter = 'a'            female : male   =     34.2 : 1.0\n",
      "             last_letter = 'k'              male : female =     31.8 : 1.0\n",
      "             last_letter = 'f'              male : female =     16.0 : 1.0\n",
      "             last_letter = 'p'              male : female =     12.6 : 1.0\n",
      "             last_letter = 'v'              male : female =      9.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#找出最有資訊的特徵\n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 當使用List儲存從龐大的文集所產生每個物件包含的特徵時，會佔用空間\n",
    "### 所以我們可以使用函式nltk.classify.apply_features，可以回傳像List且包含物件和特徵的資料且不佔用空間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'last_letter': 'n'}, 'male'), ({'last_letter': 'i'}, 'female'), ...]\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import apply_features\n",
    "\n",
    "train_set = apply_features(gender_features, labeled_names[500:])\n",
    "test_set = apply_features(gender_features, labeled_names[:500])\n",
    "\n",
    "print(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2   Choosing The Right Features\n",
    "### 為分類器選擇好的特點，可以讓準確度更高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gender_features2(name):\n",
    "    features = {}\n",
    "    features[\"first_letter\"] = name[0].lower()\n",
    "    features[\"last_letter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count({})\".format(letter)] = name.lower().count(letter)\n",
    "        features[\"has({})\".format(letter)] = (letter in name.lower())\n",
    "    return features\n",
    "\n",
    "gender_features2('John') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'count(a)': 0,'count(b)': 0,'count(c)': 0,'count(d)': 0,'count(e)': 0,'count(f)': 0,'count(g)': 0,'count(h)': 1,'count(i)': 0,'count(j)': 1,'count(k)': 0,'count(l)': 0,'count(m)': 0,'count(n)': 1,'count(o)': 1,'count(p)': 0,'count(q)': 0,'count(r)': 0,'count(s)': 0,'count(t)': 0,'count(u)': 0,'count(v)': 0,'count(w)': 0,'count(x)': 0,'count(y)': 0,'count(z)': 0,'first_letter': 'j','has(a)': False,'has(b)': False,\n",
    " 'has(c)': False,'has(d)': False,'has(e)': False,'has(f)': False,'has(g)': False,'has(h)': True,'has(i)': False,'has(j)': True,'has(k)': False,'has(l)': False,'has(m)': False,'has(n)': True,'has(o)': True,'has(p)': False,'has(q)': False,'has(r)': False,'has(s)': False,'has(t)': False,'has(u)': False,'has(v)': False,'has(w)': False,'has(x)': False,'has(y)': False,'has(z)': False,'last_letter': 'n'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 但是，選取特點的時候也要適量——如果要選取的特點太多的時候，演算法會更加依賴訓練資料的特點，可能會讓新例子歸納得較差\n",
    "### 這樣的問題稱為 overfitting，在小的訓練集當中overfitting會更加顯著"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.762\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(gender_features2(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 從以上的範例來看，gender_features2雖然取得較多的特點，但是產生出來的準確度比只取名字最後一個字元的gender_features還低"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 當選取初始的特點時，error analysis是個可以很有效率去修正的方法\n",
    "### 一開始我們選取可以用來製造模型且包含文集data的development set，development set再被分成training set和dev-test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_names用來訓練模型，devtest_names用來實作error analysis\n",
    "train_names = labeled_names[1500:]\n",
    "devtest_names = labeled_names[500:1500]\n",
    "test_names = labeled_names[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文集被分成不同的集合\n",
    "![Imgur](https://i.imgur.com/vLxZNXu.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.738\n"
     ]
    }
   ],
   "source": [
    "train_set = [(gender_features(n), gender) for (n, gender) in train_names]\n",
    "devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]\n",
    "test_set = [(gender_features(n), gender) for (n, gender) in test_names]\n",
    "\n",
    "#使用train_set來訓練分類器\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "#使用devtest_set來測試分類器\n",
    "print(nltk.classify.accuracy(classifier, devtest_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#將猜錯的名字存進errors\n",
    "errors = []\n",
    "for (name, tag) in devtest_names:\n",
    "    guess = classifier.classify(gender_features(name))\n",
    "    if guess != tag:\n",
    "        errors.append( (tag, guess, name) )\n",
    "#print(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "('gender : male', 'guess : female', 'Mortie'), <br>\n",
    "('gender : male', 'guess : female', 'Bartie'),<br>\n",
    "('gender : male', 'guess : female', 'Jackie'),<br>\n",
    "('gender : male', 'guess : female', 'Jeremie'),<br>\n",
    "('gender : male', 'guess : female', 'Charlie'),<br>\n",
    "<br>\n",
    "('gender : female', 'guess : male', 'Dorian'),<br>\n",
    "('gender : female', 'guess : male', 'Christan'), <br>\n",
    "('gender : female', 'guess : male', 'Jillian'),<br>\n",
    "('gender : female', 'guess : male', 'Lilyan')<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分類器將名字最後一個字元為e的都歸類為female，但是從結果可以看出最後兩個字元為ie的多為男性；而姓名最後兩個字元為an的通常也為女性\n",
    "### 我們從錯誤的結果可以得出取姓名的最後兩個字元為特點也許會較準確"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.763\n"
     ]
    }
   ],
   "source": [
    "#定義取姓名最後兩個字元作為特點的函式\n",
    "def gender_features3(word):\n",
    "    return {'suffix1': word[-1:],\n",
    "            'suffix2': word[-2:]}\n",
    "\n",
    "train_set = [(gender_features3(n), gender) for (n, gender) in train_names]\n",
    "devtest_set = [(gender_features3(n), gender) for (n, gender) in devtest_names]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, devtest_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 取姓名的最後兩個字元作為特點後再對分類器進行訓練，再將devtest_set做預測，預測的準確值提高了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3   Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將文件movie_reviews所有正面和負面影評載入，找出reviews中最常出現的前2000個token存成word_features<br>\n",
    " 檢查word_features中的token是否出現在正面或負面的評語中，得出來的結果作為訓練集然後丟入分類器<br>\n",
    " 最後可以從評語中包含或不包含的token來預測出是否為pos或neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 找出最常出現的前2000個token，存成all_words\n",
    "all_words = nltk.FreqDist(w.lower()for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:2000]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "#找出文件是否包含word_features內的字\n",
    "#rint(document_features(movie_reviews.words('pos/cv957_8737.txt'))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg \n",
      "\n",
      "Most Informative Features\n",
      "    contains(schumacher) = True              neg : pos    =      7.4 : 1.0\n",
      "        contains(wasted) = True              neg : pos    =      7.4 : 1.0\n",
      " contains(unimaginative) = True              neg : pos    =      7.0 : 1.0\n",
      "        contains(turkey) = True              neg : pos    =      6.8 : 1.0\n",
      "     contains(atrocious) = True              neg : pos    =      6.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# d為文件內的文字set，c為pos或neg，找出pos或neg的文件內含有的字進行訓練\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "love = {'contains(i)': True, 'contains(love)': True, 'contains(this)': True, 'contains(movie)': True}\n",
    "hate = {'contains(i)': True, 'contains(hate)': True, 'contains(this)': True, 'contains(movie)': True}\n",
    "print(classifier.classify(hate),'\\n')\n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4   Part-of-Speech Tagging\n",
    "訓練出一個分類器來找出最有意義的詞尾來作為TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 分別從brown文字的每個token的最後一個、最後兩個、最後三個字元計算頻率，找出最多出現的詞尾\n",
    "from nltk.corpus import brown\n",
    "suffix_fdist = nltk.FreqDist()\n",
    "for word in brown.words():\n",
    "    word = word.lower()\n",
    "    suffix_fdist[word[-1:]] += 1\n",
    "    suffix_fdist[word[-2:]] += 1\n",
    "    suffix_fdist[word[-3:]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', ',', '.', 's', 'd', 't', 'he', 'n', 'a', 'of', 'the', 'y', 'r', 'to', 'in', 'f', 'o', 'ed', 'nd', 'is', 'on', 'l', 'g', 'and', 'ng', 'er', 'as', 'ing', 'h', 'at', 'es', 'or', 're', 'it', '``', 'an', \"''\", 'm', ';', 'i', 'ly', 'ion', 'en', 'al', '?', 'nt', 'be', 'hat', 'st', 'his', 'th', 'll', 'le', 'ce', 'by', 'ts', 'me', 've', \"'\", 'se', 'ut', 'was', 'for', 'ent', 'ch', 'k', 'w', 'ld', '`', 'rs', 'ted', 'ere', 'her', 'ne', 'ns', 'ith', 'ad', 'ry', ')', '(', 'te', '--', 'ay', 'ty', 'ot', 'p', 'nce', \"'s\", 'ter', 'om', 'ss', ':', 'we', 'are', 'c', 'ers', 'uld', 'had', 'so', 'ey']\n"
     ]
    }
   ],
   "source": [
    "# common_suffixes儲存前100名最常出現的詞尾\n",
    "common_suffixes = [suffix for (suffix, count) in suffix_fdist.most_common(100)]\n",
    "print(common_suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#從建立好的詞尾的list來檢查傳送過來的word的詞尾是否符合\n",
    "def pos_features(word):\n",
    "    features = {}\n",
    "    for suffix in common_suffixes:\n",
    "        features['endswith({})'.format(suffix)] = word.lower().endswith(suffix)\n",
    "    return features\n",
    "\n",
    "#print(pos_features('suffixe'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6270512182993535\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NNS'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 從文集brown找出種類為news的tagged words(裡面存的是詞&詞性)\n",
    "\n",
    "tagged_words = brown.tagged_words(categories='news')\n",
    "\n",
    "# 找出詞尾是否符合common_suffixes\n",
    "featuresets = [(pos_features(n), g) for (n,g) in tagged_words]\n",
    "\n",
    "# 將size縮成原本的十分之一，不然太大了\n",
    "size = int(len(featuresets) * 0.1)\n",
    "\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "\n",
    "#訓練一個新的DecisionTree分類器\n",
    "classifier = nltk.DecisionTreeClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "classifier.classify(pos_features('cats'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 決策樹比較好的特點為他們較容易理解，還能使用NLTK以pseudocode印出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if endswith(the) == False: \n",
      "  if endswith(,) == False: \n",
      "    if endswith(s) == False: \n",
      "      if endswith(.) == False: return '.'\n",
      "      if endswith(.) == True: return '.'\n",
      "    if endswith(s) == True: \n",
      "      if endswith(is) == False: return 'PP$'\n",
      "      if endswith(is) == True: return 'BEZ'\n",
      "  if endswith(,) == True: return ','\n",
      "if endswith(the) == True: return 'AT'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classifier.pseudocode(depth=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5   Exploiting Context\n",
    "可以從單一token獲得的特點並不多，若是token的前後文通常為tag的準確度提供強大的線索<br>\n",
    "舉例來說，fly如果前面是a的話，我們可以得知這個fly的詞性是noun而並非verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prev-word': 'an', 'suffix(1)': 'n', 'suffix(2)': 'on', 'suffix(3)': 'ion'}"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果傳進來的參數是0的話，代表這個詞是開頭\n",
    "# 獲取sentence的最後1，2，3字元\n",
    "def pos_features(sentence, i):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        # 獲得前一個token\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "    return features\n",
    "\n",
    "print(brown.sents()[0])\n",
    "pos_features(brown.sents()[0], 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'suffix(1)': 'n', 'suffix(2)': 'on', 'suffix(3)': 'ton', 'prev-word': 'The'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NN'"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 句子裡每個token的詞性\n",
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "\n",
    "featuresets = []\n",
    "for tagged_sent in tagged_sents:\n",
    "    # 將原本有詞性的句子去掉詞性，剩下一般的sentence\n",
    "    untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "    #print(tagged_sent)\n",
    "    #print(untagged_sent)\n",
    "    \n",
    "    #為句子加上索引，i總共跑了一個句子的token的數量\n",
    "    for i, (word, tag) in enumerate(tagged_sent):\n",
    "        #print(word, tag)\n",
    "        #featuresets儲存token的suffix和前一個token儲存成tuple\n",
    "        featuresets.append( (pos_features(untagged_sent, i), tag))\n",
    "\n",
    "#取一部分的featuresets作為train_set和test_set，作為分類器的訓練集和測試集\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "print(test_set[1][0])\n",
    "classifier.classify(test_set[1][0])\n",
    "#nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6   Sequence Classification\n",
    "將獲取特點的函數加入一個參數history，裡面包含了句子中每個token的tag，且對應句子中每個token，根據目前token的上一個token及上一個token的tag來預測目前token本身的tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_features(sentence, i, history):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                 \"suffix(2)\": sentence[i][-2:],\n",
    "                 \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "        features[\"prev-tag\"] = \"<START>\"\n",
    "    else:\n",
    "        #prev-word和prev-tag分別儲存token的前一個token和tag\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "        features[\"prev-tag\"] = history[i-1]\n",
    "    return features\n",
    "\n",
    "class ConsecutivePosTagger(nltk.TaggerI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            #將句子中每個有tag的token去掉token\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = pos_features(untagged_sent, i, history)\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = pos_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n",
    "    \n",
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]\n",
    "tagger = ConsecutivePosTagger(train_sents)\n",
    "print(tagger.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.   Further Examples of Supervised Classification\n",
    "### 2.1   Sentence Segmentation\n",
    "要如何分割句子，簡單來說，就是用標點符號來做為分割句子的依據<BR>\n",
    "判斷哪些標點符號是一個句子的末端<BR><BR>\n",
    "底下範例來說明哪些標點符號是一段句子的結尾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "sents = nltk.corpus.treebank_raw.sents() # 抓出某篇文章且已經分割成句子\n",
    "tokens = []\n",
    "boundaries = set()\n",
    "offset = 0\n",
    "for sent in sents:\n",
    "    tokens.extend(sent) # 先還原所有的句子變成一篇文章，並分成一個一個token\n",
    "    offset += len(sent) # 計算目前tokens變數的長度總和\n",
    "    boundaries.add(offset-1) # 紀錄每個句子末端的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.', 'START'], ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov', '.', '29', '.'], ...]\n"
     ]
    }
   ],
   "source": [
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定義標點符號所擁有的feature\n",
    "def punct_features(tokens, i):\n",
    "    return {'next-word-capitalized': tokens[i+1][0].isupper(), # 判斷下個token第一個字是否為大寫\n",
    "            'prev-word': tokens[i-1].lower(), # 顯示前一個token\n",
    "            'punct': tokens[i], # 顯示目前的標點符號\n",
    "            'prev-word-is-one-char': len(tokens[i-1]) == 1} #判斷前一個token的長度是否長度為1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# featuresets為所有標點符號的feature\n",
    "featuresets = [(punct_features(tokens, i), (i in boundaries))\n",
    "               for i in range(1, len(tokens)-1)\n",
    "               if tokens[i] in '.?!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了標點符號的feature set，就能夠丟進去model訓練，做出一個分類器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.936026936026936"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set) # 訓練model\n",
    "nltk.classify.accuracy(classifier, test_set) # 評估精準度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了一個model能得知哪些標點符號是否為句子的結尾<BR>\n",
    "就能夠做出一個斷句的function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def segment_sentences(words):\n",
    "    start = 0\n",
    "    sents = []\n",
    "    for i, word in enumerate(words):\n",
    "        if word in '.?!' and classifier.classify(punct_features(words, i)) == True:\n",
    "            sents.append(words[start:i+1])\n",
    "            start = i+1\n",
    "    if start < len(words):\n",
    "        sents.append(words[start:])\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['how', 'are', 'you', '?'], ['I', 'am', 'fine', ',', 'thx', '.', ' ']]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['how', 'are', 'you','?','I','am','fine',',','thx','.',' ']\n",
    "segment_sentences(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2   Identifying Dialogue Act Types\n",
    "要辨識對話中言語的對話行為，理解對話是重要的第一步<BR><BR>\n",
    "NPS corpus是由10000筆以上的message數據所組成<BR>\n",
    "這些數據都已經被標上了標籤(label)，標籤總共有15種type<BR>\n",
    "像是\"Statement\", \"Emotion\", \"ynQuestion\", \"Continuer.\"...等等<BR><BR>\n",
    "底下就要做出一個分類器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# xml_posts()會回傳一個XML的資料\n",
    "posts = nltk.corpus.nps_chat.xml_posts()[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/6DlJ0Gx.jpg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 此function為一個feature extractor\n",
    "def dialogue_act_features(post):\n",
    "    features = {}\n",
    "    # nltk.word_tokenize()會將string做斷詞\n",
    "    for word in nltk.word_tokenize(post):\n",
    "        features['contains({})'.format(word.lower())] = True\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.668\n"
     ]
    }
   ],
   "source": [
    "# featuresets裡面包含所有對話的feature，以及每個feature set所對應的action_type(label)\n",
    "featuresets = [(dialogue_act_features(post.text), post.get('class'))\n",
    "               for post in posts]\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3   Recognizing Textual Entailment\n",
    "RTE目的，就是決定某個Text T是否包含著其他含意的text，這個text叫做hypothesis。<BR>\n",
    "底下有個範例:<BR><BR>\n",
    "    T代表原本的text，H代表T的意涵。<BR>\n",
    "    結果為true代表T跟H的含意是相同的，反之為false\n",
    "<img src=\"https://i.imgur.com/vscLDy3.jpg\"><BR>\n",
    "對於RTE，可以將它看成是一個分類的工作，來預測每一對的Text/Hypothesis之間的關係是True/False。<br>\n",
    "在理想的case中，如果有個訊息，出現在Hypothesis以及在原本的text中，那這個訊息就是個entailment。<BR>\n",
    "相反的，某個訊息只出現在Hypothesis而沒有出現在原本的text中，那這個訊息就不是個entailment。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rte_features(rtepair):\n",
    "    extractor = nltk.RTEFeatureExtractor(rtepair)\n",
    "    features = {}\n",
    "    features['word_overlap'] = len(extractor.overlap('word'))\n",
    "    features['word_hyp_extra'] = len(extractor.hyp_extra('word'))\n",
    "    features['ne_overlap'] = len(extractor.overlap('ne'))\n",
    "    features['ne_hyp_extra'] = len(extractor.hyp_extra('ne'))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text => {'together', 'meeting', 'republics', 'Davudi', 'that', 'Asia', 'four', 'Russia', 'Parviz', 'at', 'SCO', 'former', 'China', 'fledgling', 'Organisation', 'fight', 'binds', 'Soviet', 'was', 'Shanghai', 'Co', 'Iran', 'terrorism', 'association', 'central', 'representing', 'operation'} \n",
      "\n",
      "hyp => {'SCO', 'China', 'fledgling', 'fight', 'member', 'Russia'} \n",
      "\n",
      "overlap_word => {'fight', 'fledgling'} \n",
      "\n",
      "overlap_ne => {'Russia', 'China', 'SCO'} \n",
      "\n",
      "hyp_extra_word => {'member'} \n",
      "\n",
      "hyp_extra_ne => set()\n"
     ]
    }
   ],
   "source": [
    "rtepair = nltk.corpus.rte.pairs(['rte3_dev.xml'])[33]\n",
    "extractor = nltk.RTEFeatureExtractor(rtepair)\n",
    "'''\n",
    "因為並不是所有單詞都是重要的，\n",
    "所以NLTK作者認為有關人名、組織、地點的單詞可能會是比較重要的資訊。\n",
    "所以作者將這些重要單詞做了些區分:\n",
    "'ne'會取出所有有關人名、組織、地點的單詞\n",
    "'word'則是取出除了ne取出的單字\n",
    "'''\n",
    "print(\"text =>\",extractor.text_words,\"\\n\")\n",
    "print(\"hyp =>\",extractor.hyp_words,\"\\n\")\n",
    "print(\"overlap_word =>\",extractor.overlap('word'),\"\\n\")\n",
    "print(\"overlap_ne =>\",extractor.overlap('ne'),\"\\n\")\n",
    "print(\"hyp_extra_word =>\",extractor.hyp_extra('word'),\"\\n\")\n",
    "print(\"hyp_extra_ne =>\",extractor.hyp_extra('ne'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/SmGB1O1.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 3 Evaluation\n",
    "### 3.1   The Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "測試資料集必須要跟訓練資料集是不同的資料，但是格式相同<BR>\n",
    "否則評估出來的結果不會是可靠的<BR>\n",
    "<BR>\n",
    "在設計test set數量也要注意<BR>\n",
    "如果classification tasks中擁有少量且平衡的label以及多樣性test set的話，大約只需要100實例來測試就夠了<BR>\n",
    "但是，如果classification tasks擁有大量的label或是包含一些出現次數較少的label，那麼test set的內容就必須確保每一個label出現次數至少要超過50次才能做有意義的評估。<BR>\n",
    "此外，如果有個可取得的極大數據，使用整體的10%來當作test set來評估是很安全的<BR><BR>\n",
    "另外的考量，如果test set與development set的相似過太相近，評估的結果可能無法推廣到其他不同的data set。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code說明trainning set與test set取自於同一個doc(較差)\n",
    "import random\n",
    "from nltk.corpus import brown\n",
    "tagged_sents = list(brown.tagged_sents(categories='news'))\n",
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_set, test_set = tagged_sents[size:], tagged_sents[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code說明trainning set與test set取自於不同doc(較優)\n",
    "file_ids = brown.fileids(categories='news') # file_ids包含了許多文集\n",
    "size = int(len(file_ids) * 0.1)\n",
    "train_set = brown.tagged_sents(file_ids[size:])\n",
    "test_set = brown.tagged_sents(file_ids[:size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了test set就要來計算model的正確率是多少了\n",
    "NLTK提供了nltk.classify.accuracy()來計算分類器的精準度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Precision and Recall\n",
    "並不是所有的accuracy都是可靠的，有些甚至會誤導我們，像是search、資料檢索的部分<BR>\n",
    "因為網路上的文章大多跟自己搜尋的關鍵字不相干，所以對於label都會被標記為'不相干'的精準度非常接近100%<BR>\n",
    "所以有其他方法來評估這一類的model。<BR>\n",
    "<img src=\"https://i.imgur.com/DsrpBx8.jpg\" width=\"600\" align=\"center\"/><BR>\n",
    "\n",
    "分為4種狀態:<BR>\n",
    "1. True positives: 被檢索到的item與預期的相關<BR>\n",
    "2. True negatives: 未被檢索到的item與預期的不相關<BR>\n",
    "3. False positives: 被檢索到的item與預期的不相關<BR>\n",
    "4. False negatives: 未被檢索到的item與預期的相關<BR>\n",
    "\n",
    "有了上述的這些numbers，就能定義底下公式<BR>\n",
    "<ul>\n",
    "    <li>Precision : 實際被檢索到的item中，正確被檢索的item比例，TP/(TP+FP)。</li>\n",
    "    <li>Recall : 所有應該被檢索到的item，正確被檢索的item比例，TP/(TP+FN)。</li>\n",
    "    <li>F-Measure : 結合了Precision與Recall的分數，(2 × Precision × Recall) / (Precision + Recall)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3   Confusion Matrices\n",
    "confusion matrix是一個table，用來判斷每個item精確度以及錯誤率<BR>\n",
    "在table對角線代表著預測的精準度，非對角線的部分就是錯誤率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |                                         N                      |\n",
      "    |      N      I      A      J             N             V      N |\n",
      "    |      N      N      T      J      .      S      ,      B      P |\n",
      "----+----------------------------------------------------------------+\n",
      " NN | <11.9%>  0.0%      .   0.2%      .   0.0%      .   0.2%   0.0% |\n",
      " IN |   0.0%  <9.0%>     .      .      .   0.0%      .      .      . |\n",
      " AT |      .      .  <8.6%>     .      .      .      .      .      . |\n",
      " JJ |   1.6%      .      .  <4.0%>     .      .      .   0.0%   0.0% |\n",
      "  . |      .      .      .      .  <4.8%>     .      .      .      . |\n",
      "NNS |   1.5%      .      .      .      .  <3.3%>     .      .   0.0% |\n",
      "  , |      .      .      .      .      .      .  <4.4%>     .      . |\n",
      " VB |   0.9%      .      .   0.0%      .      .      .  <2.4%>     . |\n",
      " NP |   1.0%      .      .   0.0%      .      .      .      .  <1.9%>|\n",
      "----+----------------------------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "def tag_list(tagged_sents):\n",
    "    return [tag for sent in tagged_sents for (word, tag) in sent]\n",
    "def apply_tagger(tagger, corpus):\n",
    "    return [tagger.tag(nltk.tag.untag(sent)) for sent in corpus]\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "\n",
    "gold = tag_list(brown.tagged_sents(categories='editorial'))\n",
    "test = tag_list(apply_tagger(t2, brown.tagged_sents(categories='editorial')))\n",
    "cm = nltk.ConfusionMatrix(gold, test)\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4   Cross-Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "為了評估model，必須要保留一點資料來當作test set<BR>\n",
    "但如果test set太小，評估的結果就不是那麼準確<BR>\n",
    "相反的，如果test set過大，就代表training set會太小，model就不是那麼的好。<BR>\n",
    "可以解決的方法之一就是cross-validation，也就是使用多個來自於不同的test set來評估<BR>\n",
    "\n",
    "它的原理，將original corpus細分成N個subset，稱作folds<BR>\n",
    "在訓練model的時候，將那些不在folds的data拿去訓練，訓練好model之後再拿folds裡的data做評估<BR>\n",
    "如此一來，如果某個folds太小而導致評估的結果不是那麼可靠，但還有其他folds的輔助而形成可靠的評估數據。<BR>\n",
    "\n",
    "cross-validation另外的優點，可以讓我們查看不同的training set之間的效能差異<BR>\n",
    "舉例來說，如果N個training set中的分數差異不大，那麼評估出來的結果也是可靠的<BR>\n",
    "另一方面，如果N個training set中的分數差異很大，那麼評估的結果可能就不是可靠的。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
