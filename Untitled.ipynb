{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4   Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "決策樹是個簡單的流程圖，為input選擇label<br>\n",
    "他包含了:<br>\n",
    "決策節點：檢查特點的value<br>\n",
    "子節點：用來指派label<br>\n",
    "根節點：流程圖最初始的決策節點，根據條件來檢查input的特點且根據特點的value來決定分支，接著符合input的條件的分支往下走，我們到了新的決策節點，對input的value也有新的決定條件，接著節點的條件來選擇分支，直到抵達為input value提供label的葉節點為止"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](https://i.imgur.com/zlbT5c1.png)\n",
    "#### 範例：用名字來判斷性別的決策樹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decision stump<br>\n",
    "\n",
    "decision stump（決策樹樁）只擁有單層的決策樹，根據單一的特點來將input來指派label<br>\n",
    "比如說根據水果是否是圓形來判斷是否為蘋果<br>\n",
    "為了建立一個decision stump，必須先決定要使用什麼特徵條件；最簡單的方法就是為每個有可能的特徵條件建立decision stump，再觀察哪個最能在訓練集中擁有最高的準確度<br>\n",
    "選定了某個特點之後，可以根據訓練集中最常出現且符合value的label指派給每個input<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](https://i.imgur.com/0t7SKvo.png)\n",
    "根據上圖，他擁有根節點（也是內部節點）和終端節點（也是子節點），子節點也就是最終的分類結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1   Entropy and Information Gain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "information gain（資訊獲利）將較高同質性的資料放置於相同的類別，以產生各個節點,此種演算法依賴所謂Entropy（熵）<br>\n",
    "為了計算原本的input有多雜亂，我們先計算他們labels的entropy；若input種類越多，entropy越高；若input種類越少，entropy越低<br>\n",
    "entropy的公式：每個label的機率乘以同label的機率的log<br>\n",
    "H = −Σl |in| labelsP(l) × log2P(l)<br>\n",
    "<br>\n",
    "entropy:用來定義一份文件所包含的資訊量,度量Information,純度高者包含較少資訊，因此entropy比較小<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "\n",
    "def entropy(labels):\n",
    "    freqdist = nltk.FreqDist(labels)\n",
    "    probs = [freqdist.freq(l) for l in freqdist]\n",
    "    return -sum(p * math.log(p,2) for p in probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 female and 2 male: 1.0\n",
      "all male: 0.0\n",
      "all female: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"2 female and 2 male:\",entropy(['female', 'male', 'female', 'male']))\n",
    "print(\"all male:\",-entropy(['male', 'male', 'male', 'male']))\n",
    "print(\"all female:\",-entropy(['female', 'female', 'female', 'female'])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "決策樹會有Overfitting的問題，因為當決策樹成長到一定的數量的時候，可以用來訓練的資料會變少；我們可以修剪不會增進效率的決策節點"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6   Maximum Entropy Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "當我們需要對一個隨機事件的機率分布進行預測時，我們的預測應當滿足全部已知的條件，而對未知的情況不要做任何主觀假設。（不做主觀假設這 點很重要。）在這種情況下，機率分布最均勻，預測的風險最小<br>\n",
    "範例：我們要為某個詞從10個可能的詞義中挑選出正確的詞義（標記A-J），一開始我們不知道詞或詞義是什麽，從下圖可以看到10個詞義的分佈機率\n",
    "![Imgur](https://i.imgur.com/23mxkWu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們很有可能選擇(i)，因為在缺乏資訊的情況下，沒有理由認定某個詞義有可能的機率大於其他的<br>\n",
    "(i)的分佈比起其他兩個較公平，且由於種類很多，entropy會較高<br>\n",
    "Maximum Entropy Principle主張選擇擁有最高entropy的分佈<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
